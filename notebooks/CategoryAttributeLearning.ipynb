{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook performs the Simulation Experiments of Section III-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from copy import copy\n",
    "from nltk.corpus import wordnet as wn\n",
    "import matplotlib.pyplot as plt\n",
    "import anytree as at\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "import pandas as pd\n",
    "from matplotlib2tikz import save as tikz_save\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from caal import attributelearning as al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the Entity-Category Tree from AwA2 and WordNet.\n",
    "\n",
    "(Code assumes you're running this notebook from the notebooks folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = list()\n",
    "entities_id = list()\n",
    "dataset_name = 'AwA2'\n",
    "\n",
    "with open('../dataset/'+ dataset_name + '/classes_wn.txt', 'r') as f:\n",
    "      for line in f:\n",
    "            entity = line.split()[1].replace('+','_')\n",
    "            entity_id = int(line.split()[0]) - 1\n",
    "            entities.append(entity)\n",
    "            entities_id.append(entity_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = al.CategoryTree('mammal.n.01', similarity_tree_gamma=0.7)\n",
    "ct.add_leaves(entities)\n",
    "ct.simplify_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = list()\n",
    "attributes_id = list()\n",
    "with open('../dataset/'+ dataset_name + '/predicates.txt', 'r') as f:\n",
    "      for line in f:\n",
    "            attribute = line.split()[1]\n",
    "            attribute_id = int(line.split()[0]) - 1\n",
    "            attributes.append(attribute)\n",
    "            attributes_id.append(attribute_id)\n",
    "full_table = np.loadtxt(open('../dataset/'+ dataset_name + '/predicate-matrix-binary.txt', 'r'))\n",
    "binary_table = full_table[np.asarray(entities_id, dtype=int), :]\n",
    "binary_table = binary_table[:, np.asarray(attributes_id, dtype=int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(at.RenderTree(ct.root, style=at.ContStyle()).by_attr(\"description\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = at.Walker()\n",
    "paths = list()\n",
    "for entity in entities:\n",
    "    for other in entities:\n",
    "        if entity is not other:\n",
    "            paths.append(w.walk(ct.node_dictionary[ct.leaves_to_wn[entity]],ct.node_dictionary[ct.leaves_to_wn[other]]))\n",
    "            path = paths[-1]\n",
    "            # print(\"distance {}-{}: {}\".format(entity, other, (len(path[0]) + len(path[2]))))\n",
    "distances = [(len(path[0]) + len(path[2])) for path in paths]\n",
    "\n",
    "min_distance = min(distances)\n",
    "max_distance = max(distances)\n",
    "print('min distance between entities: {}'.format(min_distance))\n",
    "print('max distance between entities: {}'.format(max_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Testing the method assumption*: entities in same categories share attributes values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypo_goodness = dict()\n",
    "categories = list()\n",
    "\n",
    "for attribute_id, attribute in enumerate(attributes):\n",
    "    nodes = [node for node in at.LevelOrderIter(ct.root)]\n",
    "    for node in nodes:\n",
    "        if not node.is_leaf: # if we are not in a leaf, we must be in a category\n",
    "            if node.name not in categories:\n",
    "                categories.append(node.name)\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            descendent = node.descendants\n",
    "            for desc in descendent:\n",
    "                if desc.is_leaf:\n",
    "                    if binary_table[entities.index(desc.wn_synset.lemma_names()[0].lower()),attribute_id]:\n",
    "                        pos += 1\n",
    "                    else:\n",
    "                        neg += 1\n",
    "            hypo_goodness[attribute, node.name,'pos'] = pos\n",
    "            hypo_goodness[attribute, node.name,'neg'] = neg\n",
    "            pos = pos/(neg+pos)\n",
    "            hypo_goodness[attribute, node.name,'var'] = pos*(1-pos)\n",
    "            # hypo_goodness[attribute, node.name,'ent'] = - pos*np.log(pos) - (1-pos)*np.log(1- pos)\n",
    "            \n",
    "hypo_goodness_mat = np.zeros([len(attributes),len(ct.category_dictionary.items())])\n",
    "\n",
    "attributes_index_dataframe = dict()\n",
    "\n",
    "for category_id, category in enumerate(categories):\n",
    "    for attribute_id, attribute in enumerate(attributes):\n",
    "        hypo_goodness_mat[attribute_id, category_id] = hypo_goodness[attribute, category,'var']\n",
    "        attributes_index_dataframe[attribute_id] = attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "\n",
    "df = pd.DataFrame(hypo_goodness_mat)\n",
    "df.columns = categories\n",
    "\n",
    "df.rename(index=attributes_index_dataframe,inplace=True)\n",
    "df.round(2)\n",
    "df.style.background_gradient(cmap=cm)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_response_per_attribute = np.mean(binary_table,axis=0)\n",
    "mean_response_per_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate 4 Active Learners on the entire dataset:\n",
    "- Random\n",
    "- Learner C: greedy lowest entropy (closest to the decision boundary)\n",
    "- Learner M: query similarity (closest to previous query)\n",
    "- Learner H: hybrid greedy-similarity (phi at 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute Query Similarity (Eq. 6)\n",
    "\"\"\"\n",
    "def query_similarity(ct, current_index, previous_index, entity_list):\n",
    "    gamma = 0.7\n",
    "    similarity = np.exp(-gamma*ct.entities_distance(current_index,previous_index))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERAL PARAMETERS \n",
    "attribute_index = 0 # starting from which attribute\n",
    "attribute_number = len(attributes) # number of attributes to learn\n",
    "trials = 5 # number of run for each learner and attribute\n",
    "hard_performance_threshold = 0.49 # decides when to make the hard decision\n",
    "\n",
    "entropies = dict() # analysing entropy values for the greedy learner\n",
    "\n",
    "performances = dict()\n",
    "hard_performances = dict()\n",
    "query_similarities = dict()\n",
    "learners = ['random','similar','greedy','hybrid']\n",
    "\n",
    "for learner in learners:\n",
    "    performances[learner] = dict()\n",
    "    hard_performances[learner] = dict()\n",
    "    query_similarities[learner] = dict()\n",
    "    \n",
    "    print('======================== Learning with {} learner'.format(learner))\n",
    "    \n",
    "    for attribute in range(attribute_index,attribute_index + attribute_number):\n",
    "        ct.reset_learning()\n",
    "        \n",
    "        shuffled_entities = copy(entities)\n",
    "        perf = -1*np.ones([len(entities),trials]) # performace over time\n",
    "        hard_perf = np.zeros([len(entities),trials]) # hard performance over time\n",
    "        qsim = np.zeros([len(entities),trials]) # similarity over time\n",
    "        learned = -1*np.ones([len(entities),trials]) # vector of the learned attributes, ordered as AwA2\n",
    "        entr = list()\n",
    "        \n",
    "        print('============ Learning about {}:{} with {}'.format(attribute,attributes[attribute], learner))\n",
    "        \n",
    "        for trial in range(0, trials):\n",
    "            print('{}: run {}'.format(learner,trial + 1))\n",
    "            previous_index = None\n",
    "            ct.reset_learning()\n",
    "            shuffled_entities = copy(entities)\n",
    "            random.shuffle(shuffled_entities)\n",
    "\n",
    "            while len(shuffled_entities) != 0:\n",
    "                # select question\n",
    "                if learner is 'ere':     \n",
    "                    awa_index = ct.select_ere_query(shuffled_entities)\n",
    "                elif learner is 'greedy':\n",
    "                    awa_index, eee = ct.select_greedy_query(shuffled_entities)\n",
    "                    entr.append(eee)\n",
    "                elif learner is 'similar':\n",
    "                    awa_index, ddd = ct.select_closest_query(shuffled_entities, previous_index)\n",
    "                elif learner is 'hybrid':\n",
    "                    awa_index, sss = ct.select_hybrid_query(shuffled_entities, previous_index,min_distance,max_distance,phi=0.8)\n",
    "                elif learner is 'random':\n",
    "                    awa_index = ct.leaves.index(shuffled_entities[-1])\n",
    "                    shuffled_entities.pop()\n",
    "                else:\n",
    "                    print('ERROR: unknown learner')\n",
    "                    break\n",
    "\n",
    "                # compute the question similarity\n",
    "                if previous_index is None:\n",
    "                    previous_index = awa_index\n",
    "                else:\n",
    "                    qsim[len(entities) - len(shuffled_entities) - 1, trial] = query_similarity(ct, awa_index, previous_index, entities)\n",
    "                    previous_index = awa_index\n",
    "\n",
    "                # ask question\n",
    "                print('Asking about {} and {}'.format(entities[awa_index], attributes[attribute]))\n",
    "                learned[awa_index,trial] = binary_table[awa_index, attribute]\n",
    "\n",
    "                # update backward the tree\n",
    "                ct.node_dictionary[ct.leaves_to_wn[entities[awa_index]]].push_information(learned[awa_index, trial] == 1)\n",
    "\n",
    "                # compute the current prediction power\n",
    "                predicted = np.array(learned[:,trial], copy=True)\n",
    "                hard_predicted = np.array(learned[:,trial], copy=True)\n",
    "                for i in range(0,len(predicted)):\n",
    "                    # if I don't know for sure, I predict\n",
    "                    if predicted[i]==-1:\n",
    "                        theta = ct.node_dictionary[ct.leaves_to_wn[entities[i]]].theta\n",
    "                        predicted[i] = theta if binary_table[i, attribute] == 1 else (1 - theta)\n",
    "                        if theta > 1 - hard_performance_threshold or theta < hard_performance_threshold:\n",
    "                            if theta > 1 - hard_performance_threshold:\n",
    "                                hard_predicted[i] = 1 if binary_table[i, attribute] == 1 else 0\n",
    "                            else:\n",
    "                                hard_predicted[i] = 1 if binary_table[i, attribute] == 0 else 0\n",
    "                        else:\n",
    "                            hard_predicted[i] = 0\n",
    "                    else:\n",
    "                        predicted[i] = 1\n",
    "                        hard_predicted[i] = 1\n",
    "                perf[len(entities) - len(shuffled_entities) - 1, trial] = np.sum(predicted)\n",
    "                hard_perf[len(entities) - len(shuffled_entities) - 1, trial] = np.sum(hard_predicted)\n",
    "\n",
    "        performances[learner][attribute] = perf\n",
    "        hard_performances[learner][attribute] = hard_perf\n",
    "        query_similarities[learner][attribute] = qsim\n",
    "        \n",
    "        if learner is 'greedy':\n",
    "            entropies[attribute] = entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper fuctions for plotting\n",
    "\"\"\"\n",
    "\n",
    "def compute_statistics_on_performance(performance, many_trial=False):\n",
    "    matrix = None\n",
    "    for key, value in performance.items():\n",
    "            if not many_trial:\n",
    "                if matrix is None:\n",
    "                    matrix = value\n",
    "                else:\n",
    "                    matrix = np.hstack((matrix,value))\n",
    "            else:\n",
    "                if matrix is None:\n",
    "                    matrix = np.average(value, axis=1).reshape((-1,1))\n",
    "                else:\n",
    "                    matrix = np.hstack((matrix,np.average(value, axis=1).reshape((-1,1))))\n",
    "    avg = np.average(matrix,axis=1)\n",
    "    std = np.std(matrix,axis=1)\n",
    "    return avg, std\n",
    "\n",
    "def compute_similarity_score(similarity, many_trial=False, floor=None):\n",
    "    avg = list()\n",
    "    std = list()\n",
    "    for key, value in similarity.items():\n",
    "        if not many_trial:\n",
    "            if floor is None:\n",
    "                avg.append(np.sum(value))\n",
    "            else:\n",
    "                avg.append(np.sum(value - floor))\n",
    "        else:\n",
    "            if floor is None:\n",
    "                avg.append(np.average(np.sum(value, axis=0)))\n",
    "                std.append(np.std(np.sum(value, axis=0)))\n",
    "            else:\n",
    "                avg.append(np.average(np.sum(value, axis=0) - floor))\n",
    "                std.append(np.std(np.sum(value, axis=0) - floor))\n",
    "    return np.array(avg), np.array(std)\n",
    "\n",
    "def compute_statistics_per_attribute(measure, remove_floor = 0):\n",
    "    avg = list()\n",
    "    std = list()\n",
    "    for key, value in measure.items():\n",
    "        temp = np.sum(value - remove_floor, axis=0) \n",
    "        avg.append(np.average(temp))\n",
    "        std.append(np.std(temp))\n",
    "    return np.array(avg), np.array(std)\n",
    "\n",
    "def plot_performance(avg, std, floor, plt, c, label_choice, plot_standard_deviation=True):\n",
    "    if floor is None:\n",
    "        floor = np.zeros(np.shape(avg))\n",
    "    plt.plot(range(1,len(avg)+1), avg - floor, color=c, label=label_choice)\n",
    "    if plot_standard_deviation:\n",
    "        y1 = avg + std - floor\n",
    "        y2 = avg - std - floor\n",
    "        # plt.plot(range(1,len(avg)+1), y1, color=c,linestyle=':')\n",
    "        # plt.plot(range(1,len(avg)+1), y2, color=c,linestyle=':')\n",
    "        plt.fill_between(range(1,len(avg)+1), y1, y2, where=y2 <= y1, facecolor=c, alpha=0.1,interpolate=True)\n",
    "        \n",
    "def compute_mean_and_std(measure, remove_floor = 0):\n",
    "    all_values = list()\n",
    "    for key, value in measure.items():\n",
    "        temp = np.sum(value - remove_floor, axis=0) \n",
    "        all_values.append(temp)\n",
    "    return np.mean(all_values), np.std(all_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of plots done to visualize the performance of differente learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,24))\n",
    "plt.subplot(5, 1, 1)\n",
    "\n",
    "\"\"\"\n",
    "PERFORMANCE PLOT\n",
    "\"\"\"\n",
    "colors = {'ere':'r','similar':'g','greedy':'r','random':'m', 'hybrid':'b'}\n",
    "if 'random' not in learners:\n",
    "    learners.append('random')\n",
    "plot_std = False\n",
    "\n",
    "avg_performances = dict()\n",
    "std_performances = dict()\n",
    "\n",
    "for learner in learners:\n",
    "        avg_performances[learner], std_performances[learner] = compute_statistics_on_performance(\\\n",
    "            performances[learner], many_trial=True)\n",
    "\n",
    "flat_performance = np.linspace(1 + (len(entities) - 1)/2, len(entities), num=len(entities))\n",
    "\n",
    "for learner in learners:\n",
    "    plot_performance(avg_performances[learner],std_performances[learner],flat_performance,\\\n",
    "                     plt,colors[learner],learner, plot_standard_deviation=plot_std)\n",
    "\n",
    "plt.plot(range(0,len(avg_performances[learner])), flat_performance - flat_performance, label='flat',\\\n",
    "         color='k', linestyle='--')\n",
    "plt.title('performance - improvement wrt flat performance')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.subplot(5, 1, 2)\n",
    "avg_hard_performances = dict()\n",
    "std_hard_performances = dict()\n",
    "\n",
    "for learner in learners:\n",
    "        avg_hard_performances[learner], std_hard_performances[learner] = compute_statistics_on_performance(\\\n",
    "            hard_performances[learner], many_trial=True)\n",
    "\n",
    "flat_hard_performance = np.linspace(1 + (len(entities) - 1)/2, len(entities), num=len(entities))\n",
    "\n",
    "for learner in learners:\n",
    "    plot_performance(avg_hard_performances[learner],std_hard_performances[learner],flat_hard_performance,plt,\\\n",
    "                     colors[learner],learner, plot_standard_deviation=plot_std)\n",
    "\n",
    "plt.plot(range(0,len(avg_hard_performances[learner])), flat_hard_performance - flat_hard_performance,\\\n",
    "         label='flat',color='k', linestyle='--')\n",
    "\n",
    "plt.title('hard performance - improvement wrt flat performance')\n",
    "\n",
    "\"\"\"\n",
    "PERFORMANCE FOR EACH ATTRIBUTE\n",
    "\"\"\"\n",
    "ax = plt.subplot(5,1,3)\n",
    "performance_per_attribute = dict()\n",
    "std_performance_per_attribute = dict()\n",
    "\n",
    "for learner in learners:\n",
    "        performance_per_attribute[learner], std_performance_per_attribute[learner] =\\\n",
    "        compute_statistics_per_attribute(hard_performances[learner])\n",
    "\n",
    "for learner in learners:\n",
    "    plot_performance(performance_per_attribute[learner], std_performance_per_attribute[learner], None, plt,\\\n",
    "                     colors[learner],learner, plot_standard_deviation=plot_std)\n",
    "\n",
    "plt.title('hard performance for each attribute - integral of the curve over time')\n",
    "plt.xlabel('attribute')\n",
    "\n",
    "major_ticks = np.arange(0, len(attributes), 1)\n",
    "\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.grid()\n",
    "\n",
    "\"\"\"\n",
    "QUERY SIMILARITY PLOT\n",
    "\"\"\"\n",
    "avg_query_similarities = dict()\n",
    "std_query_similarities = dict()\n",
    "\n",
    "for learner in learners:\n",
    "    avg_query_similarities[learner], std_query_similarities[learner] =\\\n",
    "    compute_statistics_on_performance(query_similarities[learner], many_trial=True)\n",
    "\n",
    "plt.subplot(5, 1, 4)\n",
    "\n",
    "for learner in learners:\n",
    "    plot_performance(avg_query_similarities[learner],std_query_similarities[learner],\\\n",
    "                     np.zeros(np.shape(avg_query_similarities[learner])),plt,colors[learner],learner,\\\n",
    "                     plot_standard_deviation=plot_std)\n",
    "\n",
    "plt.title('query similarity over time')\n",
    "plt.xlabel('number of queries')\n",
    "\n",
    "\"\"\"\n",
    "QUERY SIMILARITY for EACH ATTRIBUTE\n",
    "\"\"\"\n",
    "ax = plt.subplot(5, 1, 5)\n",
    "similarity_score = dict()\n",
    "std_score = dict()\n",
    "\n",
    "for learner in learners:\n",
    "    similarity_score[learner], std_score[learner] = compute_statistics_per_attribute\\\n",
    "    (query_similarities[learner])\n",
    "\n",
    "for learner in learners:\n",
    "    plot_performance(similarity_score[learner], std_score[learner], None, plt, colors[learner],learner,\\\n",
    "                     plot_standard_deviation=plot_std)\n",
    "\n",
    "plt.title('sum of query similarity')\n",
    "plt.xlabel('attribute')\n",
    "major_ticks = np.arange(0, len(attributes), 1)\n",
    "\n",
    "ax.set_xticks(major_ticks)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot behind Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-notebook')\n",
    "plt.figure(figsize=(12,8))\n",
    "avg_hard_performances = dict()\n",
    "std_hard_performances = dict()\n",
    "\n",
    "color_tuned = {'similar':'xkcd:jungle green','greedy':'xkcd:red orange','random':'xkcd:medium purple', 'hybrid':'xkcd:medium blue'}\n",
    "\n",
    "for learner in learners:\n",
    "        avg_hard_performances[learner], std_hard_performances[learner] = compute_statistics_on_performance(hard_performances[learner])\n",
    "\n",
    "flat_hard_performance = np.linspace(1 + (len(entities) - 1)/2, len(entities), num=len(entities))\n",
    "\n",
    "for learner in ['similar', 'greedy', 'hybrid']:\n",
    "    plot_performance(avg_hard_performances[learner],std_hard_performances[learner],flat_hard_performance,plt,color_tuned[learner],learner, plot_standard_deviation=plot_std)\n",
    "\n",
    "plt.plot(range(0,len(avg_hard_performances[learner])), flat_hard_performance - flat_hard_performance, label='flat',color='xkcd:steel grey')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(.8, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Query number')\n",
    "plt.ylabel('Correct predictions')\n",
    "\n",
    "ts = time.time()\n",
    "giorno = datetime.datetime.fromtimestamp(ts).strftime('%m%d_%H%M')\n",
    "tikz_save('plots/performance_{}.tikz'.format(giorno))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Cumulative Query Similarity score $\\mathcal{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "similarity_score = dict()\n",
    "std_score = dict()\n",
    "\n",
    "color_tuned = {'similar':'xkcd:jungle green','greedy':'xkcd:red orange','random':'xkcd:steel grey', 'hybrid':'xkcd:medium blue'}\n",
    "\n",
    "for learner in learners:\n",
    "    similarity_score[learner], std_score[learner] = compute_statistics_per_attribute\\\n",
    "    (query_similarities[learner])\n",
    "\n",
    "for learner in learners:\n",
    "    plot_performance(similarity_score[learner], std_score[learner], None, plt, color_tuned[learner],learner, plot_standard_deviation=True)\n",
    "\n",
    "plt.title('sum of query similarity')\n",
    "plt.xlabel('attribute')\n",
    "plt.xlim([1,85])\n",
    "ax = plt.gca()\n",
    "ax.grid()\n",
    "\n",
    "ts = time.time()\n",
    "giorno = datetime.datetime.fromtimestamp(ts).strftime('%m%d_%H%M')\n",
    "tikz_save('plots/similarity_{}.tikz'.format(giorno))\n",
    "\n",
    "sim_score = dict()\n",
    "std_sim_score = dict()\n",
    "for learner in learners:\n",
    "    sim_score[learner], std_sim_score[learner] = compute_mean_and_std\\\n",
    "    (query_similarities[learner])\n",
    "print(sim_score)\n",
    "print(std_sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_per_attribute = dict()\n",
    "std_performance_per_attribute = dict()\n",
    "\n",
    "flat_performance = np.linspace(1 + (len(entities) - 1)/2, len(entities), num=len(entities))\n",
    "\n",
    "floor = np.zeros((50,trials))\n",
    "for i in range(0,trials):\n",
    "    floor[:,i] = flat_performance\n",
    "\n",
    "for learner in learners:\n",
    "    avg = list()\n",
    "    std = list()\n",
    "    measure = hard_performances[learner]\n",
    "    for key, value in measure.items():\n",
    "        temp = np.sum(value - floor, axis=0)\n",
    "        assert np.sum((value - floor)[-1,:]) == 0\n",
    "        avg.append(np.average(temp))\n",
    "        std.append(np.std(temp))\n",
    "    performance_per_attribute[learner] = avg\n",
    "    std_performance_per_attribute[learner] = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "matrix = None\n",
    "for learner in learners:\n",
    "    if matrix is None:\n",
    "        matrix = np.array(performance_per_attribute[learner]).reshape(-1,1)\n",
    "    else:\n",
    "        matrix = np.hstack((matrix, np.array(performance_per_attribute[learner]).reshape(-1,1)))\n",
    "\n",
    "matrix = np.hstack((matrix, np.arange(0, len(attributes), 1).reshape(-1,1)))\n",
    "matrix = matrix[matrix[:,2].argsort()]\n",
    "\n",
    "for i, learner in enumerate(learners):\n",
    "    plt.plot(range(0,len(matrix[:,i])), matrix[:,i], color=colors[learner], label=learner)\n",
    "\n",
    "plt.title('sum of query hard performance per attribute')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('attribute from worst performing to best (according to the greedy learner)')\n",
    "major_ticks = np.arange(0, len(attributes), 1).reshape(-1,1)\n",
    "\n",
    "ax = plt.gca()\n",
    "ticks_label = matrix[:,len(learners)].tolist()\n",
    "ticks_label = [attributes[int(t)] for t in ticks_label]\n",
    "ax.set_xticks(major_ticks)\n",
    "plt.xticks(major_ticks,ticks_label,rotation='vertical')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot behind Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "matrix = None\n",
    "selected_learners = ['greedy','similar','hybrid']\n",
    "# for learner in learners:\n",
    "for learner in selected_learners:\n",
    "    if matrix is None:\n",
    "        matrix = np.array(performance_per_attribute[learner]).reshape(-1,1)\n",
    "        matrix = np.hstack((matrix, np.array(std_performance_per_attribute[learner]).reshape(-1,1)))\n",
    "    else:\n",
    "        matrix = np.hstack((matrix, np.array(performance_per_attribute[learner]).reshape(-1,1)))\n",
    "        matrix = np.hstack((matrix, np.array(std_performance_per_attribute[learner]).reshape(-1,1)))\n",
    "\n",
    "matrix = np.hstack((matrix, np.arange(0, len(attributes), 1).reshape(-1,1)))\n",
    "matrix = matrix[matrix[:,0].argsort()]\n",
    "\n",
    "for i, learner in enumerate(selected_learners):\n",
    "    plt.plot(range(0,len(matrix[:,2*i])), matrix[:,2*i], color=color_tuned[learner], label=learner)\n",
    "    avg = matrix[:,2*i]\n",
    "    std = matrix[:,2*i+1]\n",
    "    y1 = avg + std\n",
    "    y2 = avg - std\n",
    "    plt.fill_between(range(0,len(avg)), y1, y2, where=y2 <= y1, facecolor=color_tuned[learner], alpha=0.1,interpolate=True)\n",
    "\n",
    "plt.plot(range(0,len(matrix[:,2*i])), np.zeros(np.shape(matrix[:,2*i])), color='xkcd:steel grey')\n",
    "plt.title('sum of query hard performance per attribute')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('attribute from worst performing to best (according to the greedy learner)')\n",
    "major_ticks = np.arange(0, len(attributes), 1).reshape(-1,1)\n",
    "plt.xlim([0,84])\n",
    "\n",
    "ax = plt.gca()\n",
    "ticks_label = matrix[:,len(selected_learners)*2].tolist()\n",
    "ticks_label = [attributes[int(t)] for t in ticks_label]\n",
    "ax.set_xticks(major_ticks)\n",
    "plt.xticks(major_ticks,ticks_label,rotation='vertical')\n",
    "plt.grid()\n",
    "\n",
    "ts = time.time()\n",
    "giorno = datetime.datetime.fromtimestamp(ts).strftime('%m%d_%H%M')\n",
    "tikz_save('plots/cumperf_{}.tikz'.format(giorno))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
